from typing import List, Dict, Tuple, Callable, Iterable

import numpy as np
from sacrebleu import corpus_bleu
from rouge_score import rouge_scorer, scoring
from transformers import EvalPrediction, PreTrainedTokenizer

from sentence_splitter import add_newline_to_end_of_each_sentence
from Levenshtein import distance as lev_distance

# ROUGE_KEYS = ["rouge1", "rouge2", "rougeL", "rougeLsum"]
ROUGE_KEYS = ["rouge1"]


def calculate_bleu(pred_lns: List[str], tgt_lns: List[str], **kwargs) -> dict:
    """Uses sacrebleu's corpus_bleu implementation."""
    # return {"bleu": round(corpus_bleu(output_lns, [refs_lns], **kwargs).score, 4)}
    return {"bleu": round(corpus_bleu(pred_lns, [tgt_lns], **kwargs).score, 4)}


def calculate_rouge(
    pred_lns: List[str],
    tgt_lns: List[str],
    use_stemmer=True,
    rouge_keys=ROUGE_KEYS,
    return_precision_and_recall=False,
    bootstrap_aggregation=True,
    newline_sep=True,
) -> Dict:
    """Calculate rouge using rouge_scorer package.
    Args:
        pred_lns: list of summaries generated by model
        tgt_lns: list of groundtruth summaries (e.g. contents of val.target)
        use_stemmer:  Bool indicating whether Porter stemmer should be used to
        strip word suffixes to improve matching.
        rouge_keys:  which metrics to compute, defaults to rouge1, rouge2, rougeL, rougeLsum
        return_precision_and_recall: (False) whether to also return precision and recall.
        bootstrap_aggregation: whether to do the typical bootstrap resampling of scores. Defaults to True, if False
            this function returns a collections.defaultdict[metric: list of values for each observation for each subscore]``
        newline_sep:(default=True) whether to add newline between sentences. This is essential for calculation rougeL
        on multi sentence summaries (CNN/DM dataset).
    Returns:
         Dict[score: value] if aggregate else defaultdict(list) keyed by rouge_keys
    """
    scorer = rouge_scorer.RougeScorer(rouge_keys, use_stemmer=use_stemmer)
    aggregator = scoring.BootstrapAggregator()
    # for pred, tgt in zip(tgt_lns, pred_lns):
    for pred, tgt in zip(pred_lns, tgt_lns):
        # rougeLsum expects "\n" separated sentences within a summary
        if newline_sep:
            pred = add_newline_to_end_of_each_sentence(pred)
            tgt = add_newline_to_end_of_each_sentence(tgt)
        scores = scorer.score(pred, tgt)
        aggregator.add_scores(scores)

    if bootstrap_aggregation:
        result = aggregator.aggregate()
        if return_precision_and_recall:
            return extract_rouge_mid_statistics(result)  # here we return dict
        else:
            return {k: round(v.mid.fmeasure * 100, 4) for k, v in result.items()}

    else:
        return aggregator._scores  # here we return defaultdict(list)


def extract_rouge_mid_statistics(dct):
    new_dict = {}
    for k1, v1 in dct.items():
        mid = v1.mid
        new_dict[k1] = {
            stat: round(getattr(mid, stat), 4) for stat in ["precision", "recall", "fmeasure"]
        }
    return new_dict


def build_compute_metrics_fn(
    tokenizer: PreTrainedTokenizer, metric_name: str
) -> Callable[[EvalPrediction], Dict]:
    def non_pad_len(tokens: np.ndarray) -> int:
        return np.count_nonzero(tokens != tokenizer.pad_token_id)

    def decode_pred(pred: EvalPrediction) -> Tuple[List[str], List[str]]:
        pred_str = tokenizer.batch_decode(pred.predictions, skip_special_tokens=True)
        label_str = tokenizer.batch_decode(pred.label_ids, skip_special_tokens=True)
        pred_str = lmap(str.strip, pred_str)
        label_str = lmap(str.strip, label_str)
        return pred_str, label_str

    def summarization_metrics(pred: EvalPrediction) -> Dict:
        pred_lns, label_lns = decode_pred(pred)
        metric_to_ret = {}
        if metric_name == "rouge":
            metric_to_ret = calculate_rouge(pred_lns, label_lns)
        elif metric_name == "bleu":
            metric_to_ret = calculate_bleu(pred_lns, label_lns)
        elif metric_name == "levenshtein distance":
            lev_dists = []
            for i in range(len(pred_lns)):
                lev_dists.append(lev_distance(pred_lns[i], label_lns[i]))
            metric_to_ret = {"levenshtein distance": lev_dists}
        summ_len = np.round(np.mean(lmap(non_pad_len, pred.predictions)), 1)
        metric_to_ret.update({"gen_len": summ_len})
        return metric_to_ret

    return summarization_metrics


def lmap(f: Callable, x: Iterable) -> List:
    """list(map(f, x))"""
    return list(map(f, x))
